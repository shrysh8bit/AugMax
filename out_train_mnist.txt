dataset MNIST     img size 28
loss str Lambda10.0
attack str fat-1-untargeted-10-0.1
saving to ./runs/augmax_training/AugMax_results/augmax_training/MNIST/MNIST_model/fat-1-untargeted-10-0.1_Lambda10.0_e200-b100_sgd-lr0.1-m0.9-wd0.0005_cos
Running on rank 0.
Namespace(Lambda=10.0, alpha=0.1, attacker='fat', aug_severity=3, batch_size=100, data_root_path='./data', dataset='MNIST', ddp=False, ddp_backend='nccl', decay='cos', decay_epochs=[100, 150], deepaug=False, dist_url='tcp://localhost:23456', epochs=200, gpu='0', lr=0.1, mixture_depth=-1, mixture_width=3, model='ResNeXt29', momentum=0.9, node_id=0, num_nodes=1, num_workers=16, opt='sgd', resume=False, save_root_path='./runs/augmax_training/', steps=10, targeted=False, tau=1, test_batch_size=1000, wd=0.0005, widen_factor=2)
==> Loading data..MNIST
MNIST data len Train 60000    val 10000
MNIST data loaders complete
Epoch 0-0 | Train | Loss: 2.2987 (2.2973, 0.0015), SA: 0.1400, RA: 0.1400
Epoch 0-50 | Train | Loss: 2.2194 (1.9078, 0.1470), SA: 0.2402, RA: 0.1804
Epoch 0-100 | Train | Loss: 2.1035 (1.7133, 0.2101), SA: 0.3490, RA: 0.2709
Epoch 0-150 | Train | Loss: 2.0109 (1.4173, 0.2889), SA: 0.4205, RA: 0.3336
Epoch 0-200 | Train | Loss: 1.9465 (1.4110, 0.3743), SA: 0.4677, RA: 0.3754
Epoch 0-250 | Train | Loss: 1.8947 (1.2795, 0.4643), SA: 0.5047, RA: 0.4111
Epoch 0-300 | Train | Loss: 1.8566 (1.3280, 0.2982), SA: 0.5310, RA: 0.4383
Epoch 0-350 | Train | Loss: 1.8222 (1.2091, 0.4154), SA: 0.5521, RA: 0.4590
Epoch 0-400 | Train | Loss: 1.7973 (1.0526, 0.4010), SA: 0.5688, RA: 0.4748
Epoch 0-450 | Train | Loss: 1.7785 (1.1865, 0.4235), SA: 0.5806, RA: 0.4858
Epoch 0-500 | Train | Loss: 1.7638 (1.2347, 0.4078), SA: 0.5907, RA: 0.4942
Epoch 0-550 | Train | Loss: 1.7492 (1.1304, 0.4833), SA: 0.6000, RA: 0.5032
False
Epoch 0 | Validation | Time: 40.4857 | lr: 0.1 | SA: 0.8388
Epoch 1-0 | Train | Loss: 1.5230 (1.0417, 0.4813), SA: 0.7000, RA: 0.6500
Epoch 1-50 | Train | Loss: 1.5432 (1.1623, 0.4845), SA: 0.7265, RA: 0.6280
Epoch 1-100 | Train | Loss: 1.5621 (1.0588, 0.3815), SA: 0.7167, RA: 0.6144
Epoch 1-150 | Train | Loss: 1.5600 (1.0628, 0.5956), SA: 0.7189, RA: 0.6094
Epoch 1-200 | Train | Loss: 1.5543 (0.9355, 0.4129), SA: 0.7216, RA: 0.6113
Epoch 1-250 | Train | Loss: 1.5460 (0.9470, 0.3595), SA: 0.7246, RA: 0.6149
Epoch 1-300 | Train | Loss: 1.5442 (1.0933, 0.4908), SA: 0.7259, RA: 0.6167
Epoch 1-350 | Train | Loss: 1.5389 (0.9082, 0.5301), SA: 0.7282, RA: 0.6211
Epoch 1-400 | Train | Loss: 1.5391 (1.0252, 0.4542), SA: 0.7281, RA: 0.6226
Epoch 1-450 | Train | Loss: 1.5380 (1.0109, 0.4755), SA: 0.7279, RA: 0.6226
Epoch 1-500 | Train | Loss: 1.5358 (1.0613, 0.4088), SA: 0.7283, RA: 0.6229
Epoch 1-550 | Train | Loss: 1.5338 (1.0034, 0.4225), SA: 0.7304, RA: 0.6240
False
Epoch 1 | Validation | Time: 40.6583 | lr: 0.09999383162408304 | SA: 0.8562
